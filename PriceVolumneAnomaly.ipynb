{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tOUndz2bzAzZpjKFJB-5fNcUOKzC_I28",
      "authorship_tag": "ABX9TyPmKIRv7i9hwupjmMsMQmwe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hunarpreet1/TradeAnomaly/blob/main/PriceVolumneAnomaly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9eX3VQ3WRmK"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n",
        "import ta"
      ],
      "metadata": {
        "id": "3nQSQTbfXDpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import date\n",
        "\n",
        "folder_path = '/content/drive/My Drive/PriceData'"
      ],
      "metadata": {
        "id": "sz9dKgm55EDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priceData = pd.read_csv(folder_path + '/' + 'pricedata.csv')\n",
        "universe = pd.read_csv(folder_path + '/' + 'nse200universe.csv')\n",
        "nifty50 = pd.read_csv(folder_path + '/' + 'NiftyTRI_Index.csv')\n",
        "nifty200 = pd.read_csv(folder_path + '/' + 'NSE200TRI_Index.csv')"
      ],
      "metadata": {
        "id": "J9ipbasaYt51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "universe.dropna()"
      ],
      "metadata": {
        "id": "9rxoKRlfxXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# around 3% data has volume missing\n",
        "# around 0.1% data has price missing\n",
        "priceData.set_index('date', inplace=True)\n",
        "nifty50.set_index('date', inplace=True)\n",
        "nifty200.set_index('date', inplace=True)\n",
        "\n",
        "priceData.index = pd.to_datetime(priceData.index)\n",
        "nifty50.index = pd.to_datetime(nifty50.index)\n",
        "nifty200.index = pd.to_datetime(nifty200.index)\n",
        "\n",
        "priceData.dropna(inplace=True) # Think about Nans later.....\n",
        "nifty50.fillna(method=\"ffill\").fillna(method=\"bfill\", inplace=True)\n",
        "nifty200.fillna(method=\"ffill\").fillna(method=\"bfill\", inplace=True)"
      ],
      "metadata": {
        "id": "BNGvwu5HS0ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priceData[\"totalVolume\"] = priceData.groupby(level=0)[\"PX_VOLUME\"].transform(\"sum\")\n",
        "priceData[\"maxStockVolume\"] = priceData.groupby(level=0)[\"PX_VOLUME\"].transform(\"max\")\n",
        "priceData[\"volume_ratio\"] = priceData[\"PX_VOLUME\"] / priceData[\"totalVolume\"]\n",
        "priceData[\"volume_max_ratio\"] = priceData[\"PX_VOLUME\"] / priceData[\"maxStockVolume\"]\n",
        "\n",
        "priceData['returnsOpen'] = priceData.groupby('security')['PX_OPEN'].pct_change()\n",
        "priceData['returnsClose'] = priceData.groupby('security')['PX_LAST'].pct_change()\n",
        "priceData['returnsHigh'] = priceData.groupby('security')['PX_HIGH'].pct_change()\n",
        "priceData['returnsLow'] = priceData.groupby('security')['PX_LOW'].pct_change()\n",
        "priceData.dropna(inplace=True) # Think about Nans later....."
      ],
      "metadata": {
        "id": "M9ZJDhOCSzn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract candle stick/technical features\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "Da5OjUQns6f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ta\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_mfi(df, period=14, high_col=\"PX_HIGH\", low_col=\"PX_LOW\", close_col=\"PX_LAST\", volume_col=\"PX_VOLUME\", epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Calculate Money Flow Index (MFI).\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame with OHLCV data.\n",
        "    period (int): Lookback period for MFI (default=14).\n",
        "    high_col, low_col, close_col, volume_col (str): Column names for high, low, close, and volume.\n",
        "\n",
        "    Returns:\n",
        "    pd.Series: Money Flow Index values.\n",
        "    \"\"\"\n",
        "    # Typical Price\n",
        "    df[\"TP\"] = (df[high_col] + df[low_col] + df[close_col]) / 3\n",
        "\n",
        "    # Money Flow\n",
        "    df[\"MF\"] = df[\"TP\"] * df[volume_col]\n",
        "\n",
        "    # Positive & Negative Money Flow\n",
        "    df[\"Positive MF\"] = df[\"MF\"].where(df[\"TP\"].diff() > 0, 0)\n",
        "    df[\"Negative MF\"] = df[\"MF\"].where(df[\"TP\"].diff() < 0, 0)\n",
        "\n",
        "    # Money Flow Ratio\n",
        "    df[\"MFR\"] = df[\"Positive MF\"].rolling(window=period).sum() / (df[\"Negative MF\"].rolling(window=period).sum() + epsilon)\n",
        "\n",
        "\n",
        "    # Money Flow Index\n",
        "    df[\"MFI\"] = 100 - (100 / (1 + df[\"MFR\"]))\n",
        "\n",
        "    return df[\"MFI\"]\n",
        "\n",
        "def extract_features(df):\n",
        "    \"\"\"\n",
        "    Extracts technical features from OHLCV (PX_OPEN, PX_HIGH, PX_LOW, PX_LAST, PX_VOLUME) data.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame with columns ['Date', 'security', 'PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_LAST', 'PX_VOLUME'].\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with added feature columns.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # === Basic Price Features ===\n",
        "    df[\"Daily_Return\"] = df.groupby(\"security\")[\"PX_LAST\"].pct_change()\n",
        "    #df[\"Log_Return\"] = df.groupby(\"security\")[\"PX_LAST\"].apply(lambda x: np.log(x / x.shift(1)))\n",
        "    df[\"Price_Spread\"] = df[\"PX_HIGH\"] - df[\"PX_LOW\"]\n",
        "    df[\"Body_Size\"] = abs(df[\"PX_LAST\"] - df[\"PX_OPEN\"])\n",
        "    df[\"Upper_Shadow\"] = df[\"PX_HIGH\"] - df[[\"PX_OPEN\", \"PX_LAST\"]].max(axis=1)\n",
        "    df[\"Lower_Shadow\"] = df[[\"PX_OPEN\", \"PX_LAST\"]].min(axis=1) - df[\"PX_LOW\"]\n",
        "    df[\"Close_Open_Gap\"] = (df[\"PX_OPEN\"] - df.groupby(\"security\")[\"PX_LAST\"].shift(1)) / df.groupby(\"security\")[\"PX_LAST\"].shift(1)\n",
        "    df[\"Volume_Change\"] = df.groupby(\"security\")[\"PX_VOLUME\"].pct_change()\n",
        "\n",
        "    # === Trend Indicators ===\n",
        "    df[\"SMA_5\"] = df.groupby(\"security\")[\"PX_LAST\"].transform(lambda x: x.rolling(5).mean())\n",
        "    df[\"SMA_20\"] = df.groupby(\"security\")[\"PX_LAST\"].transform(lambda x: x.rolling(20).mean())\n",
        "    df[\"EMA_12\"] = ta.trend.EMAIndicator(df[\"PX_LAST\"], window=12).ema_indicator()\n",
        "    df[\"EMA_26\"] = ta.trend.EMAIndicator(df[\"PX_LAST\"], window=26).ema_indicator()\n",
        "    df[\"MACD\"] = ta.trend.MACD(df[\"PX_LAST\"]).macd()\n",
        "    df[\"MACD_Signal\"] = ta.trend.MACD(df[\"PX_LAST\"]).macd_signal()\n",
        "    df[\"RSI\"] = ta.momentum.RSIIndicator(df[\"PX_LAST\"], window=14).rsi()\n",
        "\n",
        "    # === Volatility Indicators ===\n",
        "    df[\"ATR\"] = ta.volatility.AverageTrueRange(df[\"PX_HIGH\"], df[\"PX_LOW\"], df[\"PX_LAST\"], window=14).average_true_range()\n",
        "    bollinger = ta.volatility.BollingerBands(df[\"PX_LAST\"], window=20, window_dev=2)\n",
        "    df[\"Bollinger_Upper\"] = bollinger.bollinger_hband()\n",
        "    df[\"Bollinger_Lower\"] = bollinger.bollinger_lband()\n",
        "    #df[\"Historical_Volatility\"] = df.groupby(\"security\")[\"Log_Return\"].transform(lambda x: x.rolling(20).std())\n",
        "\n",
        "    # === Momentum Indicators ===\n",
        "    df[\"Momentum_5\"] = ta.momentum.ROCIndicator(df[\"PX_LAST\"], window=5).roc()\n",
        "    df[\"Williams_%R\"] = ta.momentum.WilliamsRIndicator(df[\"PX_HIGH\"], df[\"PX_LOW\"], df[\"PX_LAST\"], lbp=14).williams_r()\n",
        "\n",
        "    # Money Flow Index\n",
        "    df[\"MFI\"] = calculate_mfi(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "# === Apply Feature Extraction to Dataset ===\n",
        "df_features = extract_features(priceData)  # df contains multiple securities\n",
        "print(df_features.head())\n"
      ],
      "metadata": {
        "id": "Aln4IykDtDJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getStockPriceVolumeData(ticker, start=None, end=None):\n",
        "  stock_data = priceData[priceData['security'] == ticker + IN_EQUITY_PREFIX]\n",
        "  if start is not None:\n",
        "    stock_data = stock_data.loc[start:]\n",
        "  if end is not None:\n",
        "    stock_data = stock_data.loc[:end]\n",
        "  return stock_data\n",
        "\n",
        "def plotPrice(ticker, priceType='PX_OPEN', start=None, end=None, dotDates=None, lineDates=None, saveFig=True, saveSuffix=''):\n",
        "  df = getStockPriceVolumeData(ticker, start=start, end=end)\n",
        "  plt.close(\"all\")  # Close any previous plots\n",
        "  fig, ax = plt.subplots(figsize=(10, 5))  # Create a new figure explicitly\n",
        "\n",
        "  ax.plot(df.index, df[priceType], label=priceType, color='black')\n",
        "\n",
        "  # Formatting\n",
        "  ax.set_xlabel(\"Date\")\n",
        "  ax.set_ylabel(priceType)\n",
        "  ax.set_title(ticker)\n",
        "  ax.legend()\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.grid()\n",
        "  ax.xaxis.set_major_locator(mdates.AutoDateLocator())  # Automatically adjusts date labels\n",
        "  fig.autofmt_xdate()  # Rotates dates for better readability\n",
        "  start = pd.Timestamp(start) if start is not None else df.index[0]\n",
        "  end = pd.Timestamp(end) if end is not None else df.index[-1]\n",
        "\n",
        "  if dotDates is not None:\n",
        "    dotDates = [date for date in dotDates if date >= start and date <= end]\n",
        "    plt.scatter(dotDates, df.loc[dotDates, priceType], color='red', zorder=3, label=\"Dots\")\n",
        "\n",
        "  if lineDates is not None:\n",
        "    lineDates = [pd.Timestamp(date) for date in lineDates if start <= pd.Timestamp(date) <= end]\n",
        "    print(lineDates)\n",
        "    for date in lineDates:\n",
        "        plt.axvline(x=date, color='blue', linestyle='--', linewidth=1, alpha=0.7, label=\"Event\" if date == lineDates[0] else \"\")\n",
        "\n",
        "  if saveFig:\n",
        "    plt.savefig(ticker + '_' + saveSuffix + '.png', bbox_inches=\"tight\")\n",
        "    #print(f\"Plot saved to {save_path}\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2vCQzxiWa-eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remove Index component from stock price"
      ],
      "metadata": {
        "id": "Yb436Pa9tEO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IN_EQUITY_PREFIX = ' IN Equity'\n",
        "CURR_UNIVERSE = [ ticker.split(' ')[0] for ticker in universe['2022-12-30'] if pd.notna(ticker) ]"
      ],
      "metadata": {
        "id": "0-cWufUx6ouO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def extend_index_composition(index_composition_df):\n",
        "    \"\"\"\n",
        "    Extend index composition backward in time before the first available date.\n",
        "    Uses the earliest available composition for all prior dates.\n",
        "\n",
        "    Parameters:\n",
        "    index_composition_df (pd.DataFrame): Index components with dates as columns and tickers as values.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Extended index composition.\n",
        "    \"\"\"\n",
        "    first_date = index_composition_df.columns.min()  # Find earliest available date\n",
        "    all_dates = pd.date_range(start=price_df.index.min(), end=index_composition_df.columns.max(), freq=\"B\")  # All business days\n",
        "\n",
        "    # Convert composition to DataFrame with dates as index\n",
        "    extended_composition = index_composition_df.T\n",
        "    extended_composition = extended_composition.reindex(all_dates, method=\"ffill\")  # Fill backward before 2012\n",
        "\n",
        "    return extended_composition.T  # Convert back to original format\n",
        "\n",
        "def compute_equal_weighted_index_returns(price_df, index_composition_df):\n",
        "    \"\"\"\n",
        "    Compute equal-weighted index returns using daily stock prices and index composition.\n",
        "\n",
        "    Parameters:\n",
        "    price_df (pd.DataFrame): OHLCV data with date index and tickers as columns (PX_LAST values).\n",
        "    index_composition_df (pd.DataFrame): Index components with dates as columns and tickers as values.\n",
        "\n",
        "    Returns:\n",
        "    pd.Series: Equal-weighted index returns over time.\n",
        "    \"\"\"\n",
        "    # Extend composition before 2012\n",
        "    index_composition_df = extend_index_composition(index_composition_df) # check with Anay on composition data before 2012....\n",
        "\n",
        "    returns = price_df.pct_change()  # Compute daily simple returns\n",
        "    index_returns = {}\n",
        "\n",
        "    for date in index_composition_df.columns:  # Loop through each date\n",
        "        if date not in returns.index:\n",
        "            continue  # Skip if no return data\n",
        "\n",
        "        tickers = index_composition_df[date].dropna().values  # Get tickers for that day\n",
        "        valid_returns = returns.loc[date, tickers].dropna()  # Get valid returns\n",
        "\n",
        "        if len(valid_returns) > 0:\n",
        "            index_returns[date] = valid_returns.mean()  # Compute equal-weighted return\n",
        "\n",
        "    return pd.Series(index_returns, name=\"Equal_Weighted_Return\")\n",
        "\n",
        "# Example Usage\n",
        "index_composition_df = extend_index_composition(index_composition_df)  # Extend composition first\n",
        "equal_weighted_index_returns = compute_equal_weighted_index_returns(price_df, index_composition_df)\n"
      ],
      "metadata": {
        "id": "5KZa4Gc9rdyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats import probplot\n",
        "\n",
        "def compute_log_returns(df):\n",
        "    \"\"\" Compute log returns for a given price series. \"\"\"\n",
        "    return np.log(df / df.shift(1))\n",
        "\n",
        "def compute_simple_returns(df):\n",
        "    \"\"\" Compute simple returns for a given price series. \"\"\"\n",
        "    return df.pct_change()\n",
        "\n",
        "def compute_idiosyncratic_returns(priceData, indexData, universe=CURR_UNIVERSE):\n",
        "    \"\"\"\n",
        "    Compute idiosyncratic returns efficiently without pivoting.\n",
        "\n",
        "    Parameters:\n",
        "    priceData (pd.DataFrame): DataFrame with columns [\"date\", \"security\", \"PX_LAST\"].\n",
        "    indexData (pd.Series): Series with index daily closing prices (indexed by date).\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Idiosyncratic returns for each security.\n",
        "    \"\"\"\n",
        "    idiosyncratic_returns = {}\n",
        "\n",
        "    # Compute log returns for index\n",
        "    index_returns = compute_log_returns(indexData).dropna()\n",
        "\n",
        "    # Iterate over each unique security\n",
        "    for security in universe:\n",
        "        print(security + \" Regression Started\")\n",
        "        sec_data = getStockPriceVolumeData(security)[\"PX_LAST\"]\n",
        "        stock_returns = compute_log_returns(sec_data).dropna()\n",
        "\n",
        "        # Align data\n",
        "        aligned_data = pd.concat([stock_returns, index_returns], axis=1, join=\"inner\").dropna()\n",
        "        if len(aligned_data) < 30:  # Skip if insufficient data\n",
        "            continue\n",
        "\n",
        "        X = sm.add_constant(aligned_data.iloc[:, 1])  # Add constant (α) for regression\n",
        "        y = aligned_data.iloc[:, 0]\n",
        "\n",
        "        # Regression: Stock Return = α + β × Index Return + ε\n",
        "        model = sm.OLS(y, X).fit()\n",
        "        predicted_returns = model.predict(X)\n",
        "\n",
        "        # Compute idiosyncratic returns (residuals)\n",
        "        residuals = y - predicted_returns\n",
        "        idiosyncratic_returns[security] = residuals\n",
        "        print(f\"{security} Regression Finished | R² = {model.rsquared:.4f}\")\n",
        "\n",
        "        # # 📊 Plot 1: Scatter Plot with Regression Line\n",
        "        # plt.figure(figsize=(10, 5))\n",
        "        # sns.regplot(x=aligned_data.iloc[:, 1], y=aligned_data.iloc[:, 0],\n",
        "        #             scatter_kws={\"alpha\": 0.5}, line_kws={\"color\": \"red\"}, ci=None)\n",
        "        # plt.xlabel(\"Index Returns\")\n",
        "        # plt.ylabel(f\"{security} Returns\")\n",
        "        # plt.title(f\"Regression of {security} on Index (α={model.params[0]:.4f}, β={model.params[1]:.4f})\")\n",
        "        # plt.grid(True)\n",
        "        # plt.show()\n",
        "\n",
        "        # # 📊 Plot 2: Residuals Distribution (Idiosyncratic Returns)\n",
        "        # plt.figure(figsize=(10, 5))\n",
        "        # sns.histplot(residuals, bins=50, kde=True)\n",
        "        # plt.axvline(0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
        "        # plt.xlabel(\"Residuals (Idiosyncratic Returns)\")\n",
        "        # plt.ylabel(\"Frequency\")\n",
        "        # plt.title(f\"Distribution of Idiosyncratic Returns for {security}\")\n",
        "        # plt.grid(True)\n",
        "        # plt.show()\n",
        "\n",
        "        # # 📊 Plot 3: Q-Q Plot for Residuals Normality Check\n",
        "        # plt.figure(figsize=(6, 6))\n",
        "        # probplot(residuals, dist=\"norm\", plot=plt)\n",
        "        # plt.title(f\"Q-Q Plot of Residuals for {security}\")\n",
        "        # plt.grid(True)\n",
        "        # plt.show()\n",
        "\n",
        "        # # 📊 Plot 4: Residuals vs Fitted Values to check heteroskedasticity\n",
        "        # plt.figure(figsize=(10, 5))\n",
        "        # plt.scatter(predicted_returns, residuals, alpha=0.5)\n",
        "        # plt.axhline(0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
        "        # plt.xlabel(\"Fitted Values (Predicted Returns)\")\n",
        "        # plt.ylabel(\"Residuals\")\n",
        "        # plt.title(f\"Residuals vs Fitted Values for {security}\")\n",
        "        # plt.grid(True)\n",
        "        # plt.show()\n",
        "\n",
        "    # Convert dictionary to DataFrame\n",
        "    idio_df = pd.DataFrame(idiosyncratic_returns)\n",
        "    return idio_df\n",
        "\n",
        "idiosyncratic_returns = compute_idiosyncratic_returns(priceData, nifty200[\"PX_LAST\"])\n",
        "print(idiosyncratic_returns.head())\n"
      ],
      "metadata": {
        "id": "2kBBoty21aty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "universe.dropna()"
      ],
      "metadata": {
        "id": "M9jwQltWtDyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaeVZjSCs4Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priceData.columns"
      ],
      "metadata": {
        "id": "2eGxt_zudLE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CURR_UNIVERSE"
      ],
      "metadata": {
        "id": "gT-e87Ff5JgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 1"
      ],
      "metadata": {
        "id": "gI-6NNLV8A4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (7,4.5) # Make the default figures a bit bigger\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "UemkSR8T0GAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "utoencoder Model: An autoencoder is implemented as a class AnomalyDetector that extends the tf.keras.models.Model class.\n",
        "The autoencoder is composed of two parts: an encoder and a decoder.\n",
        "The encoder reduces the dimensionality of the input data, and the decoder restores it to its original shape.\n",
        "'''\n",
        "\n",
        "# class AnomalyDetectorNN(Model):\n",
        "#     def __init__(self, input_dim=30):\n",
        "#         super(AnomalyDetectorNN, self).__init__()\n",
        "\n",
        "#         # Encoder: Compress input into a smaller representation\n",
        "#         self.encoder = tf.keras.Sequential([\n",
        "#             layers.Dense(128, activation=\"relu\", input_shape=(input_dim,)),  # Input shape added\n",
        "#             layers.Dense(16, activation=\"relu\")\n",
        "#         ])\n",
        "\n",
        "#         # Decoder: Reconstruct input from encoded representation\n",
        "#         self.decoder = tf.keras.Sequential([\n",
        "#             layers.Dense(128, activation=\"relu\"),\n",
        "#             layers.Dense(input_dim, activation=\"sigmoid\")  # Ensures output matches input dimension\n",
        "#         ])\n",
        "\n",
        "#     def call(self, x):\n",
        "#         encoded = self.encoder(x)   # Encode input\n",
        "#         decoded = self.decoder(encoded)  # Decode back to original shape\n",
        "#         return decoded\n",
        "\n",
        "# autoencoder = AnomalyDetectorNN(30)\n",
        "\n",
        "class AnomalyDetectorNN(Model):\n",
        "    def __init__(self, window_size=30, num_features=5):\n",
        "        super(AnomalyDetectorNN, self).__init__()\n",
        "\n",
        "        input_dim = window_size * num_features  # Flattened input size\n",
        "\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            layers.Flatten(),  # Convert (batch, 30, features) → (batch, 30*num_features)\n",
        "            layers.Dense(128, activation=\"relu\"),\n",
        "            layers.Dense(16, activation=\"relu\")\n",
        "        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            layers.Dense(128, activation=\"relu\"),\n",
        "            layers.Dense(input_dim, activation=\"sigmoid\"),  # Linear activation\n",
        "            layers.Reshape((window_size, num_features))  # Reshape back to original\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n"
      ],
      "metadata": {
        "id": "EbZlnNHg8EO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# class LSTMAutoencoder(Model):\n",
        "#     def __init__(self, input_dim=30, latent_dim=16):\n",
        "#         super(LSTMAutoencoder, self).__init__()\n",
        "\n",
        "#         # Encoder: Compress input into a smaller representation\n",
        "#         self.encoder = tf.keras.Sequential([\n",
        "#             layers.LSTM(128, activation=\"relu\", return_sequences=True, input_shape=(input_dim, 1)),\n",
        "#             layers.LSTM(latent_dim, activation=\"relu\", return_sequences=False)  # Bottleneck\n",
        "#         ])\n",
        "\n",
        "#         # Decoder: LSTM reconstructing original input\n",
        "#         self.decoder = tf.keras.Sequential([\n",
        "#             layers.RepeatVector(input_dim),  # Expands bottleneck vector across time steps\n",
        "#             layers.LSTM(128, activation=\"relu\", return_sequences=True),\n",
        "#             layers.TimeDistributed(layers.Dense(1))  # Output shape matches input (30,1)\n",
        "#         ])\n",
        "\n",
        "#     def call(self, x):\n",
        "#         encoded = self.encoder(x)  # Compress input into bottleneck representation\n",
        "#         decoded = self.decoder(encoded)  # Reconstruct sequence from bottleneck\n",
        "#         return decoded\n",
        "\n",
        "# # Example usage\n",
        "# input_dim = 30  # Sequence length (time steps)\n",
        "# latent_dim = 8  # Compressed representation\n",
        "\n",
        "# autoencoder = LSTMAutoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
        "# #autoencoder.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "\n",
        "# class LSTMAutoencoder(Model):\n",
        "#     def __init__(self, timeSteps=30, numFeatures=5, latent_dim=8):\n",
        "#         super(LSTMAutoencoder, self).__init__()\n",
        "\n",
        "#         # Encoder: Compress input into a latent representation\n",
        "#         self.encoder = tf.keras.Sequential([\n",
        "#             layers.LSTM(128, activation=\"relu\", return_sequences=True, input_shape=(timeSteps, numFeatures)),\n",
        "#             layers.LSTM(latent_dim, activation=\"relu\", return_sequences=False)  # Bottleneck\n",
        "#         ])\n",
        "\n",
        "#         # Decoder: Reconstruct original input shape (timeSteps, numFeatures)\n",
        "#         self.decoder = tf.keras.Sequential([\n",
        "#             layers.RepeatVector(timeSteps),  # Expand bottleneck across time steps\n",
        "#             layers.LSTM(128, activation=\"relu\", return_sequences=True),\n",
        "#             layers.TimeDistributed(layers.Dense(numFeatures))  # Ensure output matches input shape\n",
        "#         ])\n",
        "\n",
        "#     def call(self, x):\n",
        "#         encoded = self.encoder(x)  # Compress input into latent space\n",
        "#         decoded = self.decoder(encoded)  # Reconstruct input from latent space\n",
        "#         return decoded\n",
        "\n",
        "class LSTMAutoencoder(Model):\n",
        "    def __init__(self, timeSteps=30, numFeatures=5, latent_dim=16):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder: Compress input into a latent representation\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            layers.LSTM(128, activation=\"relu\", return_sequences=True, input_shape=(timeSteps, numFeatures)),\n",
        "            layers.LSTM(latent_dim, activation=\"relu\", return_sequences=False)  # Keep sequences\n",
        "        ])\n",
        "\n",
        "        # Decoder: Reconstruct the original input shape (timeSteps, numFeatures)\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            layers.RepeatVector(timeSteps),  # Expand latent representation\n",
        "            layers.LSTM(128, activation=\"relu\", return_sequences=True),\n",
        "            #layers.LSTM(64, activation=\"relu\", return_sequences=True),  # Extra LSTM layer\n",
        "            layers.TimeDistributed(layers.Dense(numFeatures))  # Ensure output matches input\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        encoded = self.encoder(x)  # Compress input\n",
        "        decoded = self.decoder(encoded)  # Reconstruct input\n",
        "        return decoded\n",
        "\n"
      ],
      "metadata": {
        "id": "Llp5MhHxRtgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_window_roll(data, window_size, step=1, cols_to_keep=None):\n",
        "    \"\"\"\n",
        "    Normalizes a rolling window of data using MinMaxScaler.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pd.DataFrame, with datetime index and multiple columns.\n",
        "    - window_size: int, size of rolling window.\n",
        "    - step: int, step size for moving window.\n",
        "    - cols_to_keep: list, column names to keep for normalization.\n",
        "\n",
        "    Returns:\n",
        "    - X: np.array of shape (num_samples, window_size, num_features).\n",
        "    - index_map: list of lists, mapping each index to dates.\n",
        "    \"\"\"\n",
        "\n",
        "    X = []\n",
        "    index_map = []\n",
        "\n",
        "    # If cols_to_keep is provided, filter only those columns\n",
        "    if cols_to_keep:\n",
        "        data = data[cols_to_keep]\n",
        "\n",
        "    for i in range(0, len(data) - window_size, step):\n",
        "        x_window = data.iloc[i : i + window_size]  # Extract rolling window\n",
        "        index_map.append(x_window.index.to_list())  # Store index list\n",
        "\n",
        "        # Normalize across all selected columns\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        x_window_scaled = scaler.fit_transform(x_window)  # Shape: (window_size, num_features)\n",
        "\n",
        "        X.append(x_window_scaled)\n",
        "\n",
        "    X = np.array(X)  # Shape: (num_samples, window_size, num_features)\n",
        "\n",
        "    return X, index_map\n",
        "\n"
      ],
      "metadata": {
        "id": "CUK_ofP6AFEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def split_data(X, index_map, test_start, test_end, val_start, val_end):\n",
        "    \"\"\"\n",
        "    Splits data into train, validation, and test sets based on given date ranges.\n",
        "\n",
        "    Parameters:\n",
        "    - X: np.array (N_data x seq_length x num_features), the normalized rolling window data.\n",
        "    - index_map: list of lists containing pd.Timestamp (N_data x seq_length), mapping each index in X to a sequence of dates.\n",
        "    - test_start: datetime.date or pd.Timestamp, start date for test set.\n",
        "    - test_end: datetime.date or pd.Timestamp, end date for test set.\n",
        "    - val_start: datetime.date or pd.Timestamp, start date for validation set.\n",
        "    - val_end: datetime.date or pd.Timestamp, end date for validation set.\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_val, X_test: np.array, split data (same 3D shape structure).\n",
        "    - index_train, index_val, index_test: list of lists of pd.Timestamp.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert all date inputs to pd.Timestamp for consistency\n",
        "    test_start, test_end = pd.Timestamp(test_start), pd.Timestamp(test_end)\n",
        "    val_start, val_end = pd.Timestamp(val_start), pd.Timestamp(val_end)\n",
        "\n",
        "    # Ensure index_map is not empty\n",
        "    if not index_map:\n",
        "        seq_length = X.shape[1] if X.ndim == 3 else 0\n",
        "        num_features = X.shape[2] if X.ndim == 3 else 0\n",
        "        return (np.empty((0, seq_length, num_features)),  # X_train\n",
        "                np.empty((0, seq_length, num_features)),  # X_val\n",
        "                np.empty((0, seq_length, num_features)),  # X_test\n",
        "                [], [], [])  # Empty index lists\n",
        "\n",
        "    # Extract representative date (first date of each sequence in index_map)\n",
        "    index_representative = np.array([dates[0] for dates in index_map])\n",
        "\n",
        "    # Identify test indices\n",
        "    test_mask = (index_representative >= test_start) & (index_representative <= test_end)\n",
        "    test_indices = np.where(test_mask)[0]\n",
        "    X_test = X[test_indices] if len(test_indices) > 0 else np.empty((0, X.shape[1], X.shape[2]))\n",
        "    index_test = [index_map[i] for i in test_indices] if len(test_indices) > 0 else []\n",
        "\n",
        "    # Identify validation indices\n",
        "    val_mask = (index_representative >= val_start) & (index_representative <= val_end)\n",
        "    val_indices = np.where(val_mask)[0]\n",
        "    X_val = X[val_indices] if len(val_indices) > 0 else np.empty((0, X.shape[1], X.shape[2]))\n",
        "    index_val = [index_map[i] for i in val_indices] if len(val_indices) > 0 else []\n",
        "\n",
        "    # Remaining data for training\n",
        "    train_mask = ~(test_mask | val_mask)\n",
        "    train_indices = np.where(train_mask)[0]\n",
        "    X_train = X[train_indices] if len(train_indices) > 0 else np.empty((0, X.shape[1], X.shape[2]))\n",
        "    index_train = [index_map[i] for i in train_indices] if len(train_indices) > 0 else []\n",
        "\n",
        "    return X_train, X_val, X_test, index_train, index_val, index_test\n"
      ],
      "metadata": {
        "id": "5LWJf3EMIPJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Source: https://www.tensorflow.org/tutorials/generative/autoencoder\n",
        "def predict(model, data, threshold):\n",
        "  reconstructions = model(data)\n",
        "  loss = tf.reduce_mean(tf.keras.losses.mae(reconstructions, data), axis=1)\n",
        "  return tf.math.less(loss, threshold)"
      ],
      "metadata": {
        "id": "nC_mp1k8AdAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reconstructions.shape\n",
        "#X_train.shape\n",
        "train_loss.shape"
      ],
      "metadata": {
        "id": "CVbQL06iXczx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TICKERS = CURR_UNIVERSE\n",
        "TICKERS = ['HDFC', 'RELIANCE', 'SBIN']\n",
        "PRICE_TYPE = 'PX_LAST'\n",
        "#COLS_TO_KEEP = ['PX_LAST','PX_OPEN', 'PX_HIGH', 'PX_LOW', 'volume_max_ratio' ]\n",
        "#COLS_TO_KEEP = ['PX_LAST','PX_OPEN', 'PX_HIGH', 'PX_LOW', 'volume_ratio' ]\n",
        "#COLS_TO_KEEP = ['returnsClose','returnsOpen', 'returnsHigh', 'returnsLow', 'volume_max_ratio' ]\n",
        "COLS_TO_KEEP = [ 'PX_LAST' ]\n",
        "# input_dim = 30\n",
        "# latent_dim = 8\n",
        "MODEL_INFO = \"\"\n",
        "returnsAtAnomalies = {}\n",
        "\n",
        "for ticker in TICKERS:\n",
        "  try:\n",
        "    prices = getStockPriceVolumeData(ticker)\n",
        "\n",
        "    X, index_map = normalize_window_roll(prices,30,5,COLS_TO_KEEP)\n",
        "    startTest = date(2019, 1, 1)\n",
        "    endTest = date(2021, 1, 1)\n",
        "\n",
        "    startVal = date(2014, 1, 1)\n",
        "    endVal = date(2018, 1, 1)\n",
        "\n",
        "    X_train, X_val, X_test, index_train, index_val, index_test = split_data(X, index_map, startTest, endTest, startVal, endVal)\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=2,\n",
        "                                                    mode='min')\n",
        "\n",
        "    ########################### CHANGE MDOEL HERE ###########################\n",
        "\n",
        "    autoencoder = AnomalyDetectorNN(30, 1)\n",
        "    #autoencoder = LSTMAutoencoder(30, 1, 16)\n",
        "    autoencoder.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "    history = autoencoder.fit(X_train,X_train, epochs=80,\n",
        "                        validation_data=(X_val, X_val),\n",
        "                        batch_size=16,\n",
        "                        callbacks=[early_stopping])\n",
        "\n",
        "    reconstructions = autoencoder.predict(X_train)\n",
        "    train_loss = tf.reduce_mean(tf.keras.losses.mae(reconstructions, X_train), axis=1)\n",
        "\n",
        "    plt.hist(train_loss[None,:], bins=50)\n",
        "    plt.xlabel(\"Train loss\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "\n",
        "    threshold = np.mean(train_loss) + np.std(train_loss)\n",
        "    #threshold = np.max(train_loss)\n",
        "    print(\"Threshold: \", threshold)\n",
        "\n",
        "    reconstructions = autoencoder.predict(X_test)\n",
        "    test_loss = tf.reduce_mean(tf.keras.losses.mae(reconstructions, X_test), axis=1)\n",
        "\n",
        "    plt.hist(test_loss[None, :], bins=50)\n",
        "    plt.xlabel(\"Test loss\")\n",
        "    plt.ylabel(\"No of examples\")\n",
        "    plt.show()\n",
        "\n",
        "    preds = predict(autoencoder, X_test, threshold)\n",
        "\n",
        "    anomalousIndexIds  = np.where(preds == False)[0]\n",
        "\n",
        "    dotDates = [index_test[i][-1] for i in anomalousIndexIds]\n",
        "    plotPrice(ticker, PRICE_TYPE, date(2015, 1, 1), None, dotDates, lineDates=[startTest, endTest], saveSuffix=autoencoder.__class__.__name__ + MODEL_INFO)\n",
        "\n",
        "    returns = calculate_returns(prices, dotDates)\n",
        "    returnsAtAnomalies[ticker] = returns\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing {ticker}: {e}\")\n",
        "\n",
        "returnsAtAnomaliesDf = pd.concat(\n",
        "    [df.assign(Ticker=ticker) for ticker, df in returnsAtAnomalies.items()],\n",
        "    ignore_index=True\n",
        ")\n",
        "results = evaluate_predictions(returnsAtAnomaliesDf)\n",
        "results.to_csv('results' + autoencoder.__class__.__name__ + MODEL_INFO + '.csv')\n"
      ],
      "metadata": {
        "id": "OshXmKcsAkNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_predictions(returnsAtAnomaliesDf)"
      ],
      "metadata": {
        "id": "RLaV3qbYWJ8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "returnsAtAnomaliesDf"
      ],
      "metadata": {
        "id": "Vj_LiSvLcTzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Direction of trade"
      ],
      "metadata": {
        "id": "B8CPvZKYIZrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def signalFromAnomaly(data, anomalyDates):\n",
        "    \"\"\"\n",
        "    Generate trading signals based on anomalies using trend & mean reversion filters.\n",
        "\n",
        "    Parameters:\n",
        "    data (pd.DataFrame): DataFrame with 'Close' prices and date index.\n",
        "    anomalyDates (list): List of dates (as strings) where anomalies were detected.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Original DataFrame with a new 'Signal' column.\n",
        "    \"\"\"\n",
        "    df = data.copy()\n",
        "\n",
        "    # Ensure the index is a datetime index\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Compute Moving Averages\n",
        "    df['SMA_5'] = df['PX_LAST'].rolling(window=5).mean()\n",
        "    df['SMA_20'] = df['PX_LAST'].rolling(window=20).mean()\n",
        "\n",
        "    # Compute Bollinger Bands\n",
        "    rolling_mean = df['PX_LAST'].rolling(window=20).mean()\n",
        "    rolling_std = df['PX_LAST'].rolling(window=20).std()\n",
        "    df['BB_Upper'] = rolling_mean + (2 * rolling_std)\n",
        "    df['BB_Lower'] = rolling_mean - (2 * rolling_std)\n",
        "\n",
        "    # Compute RSI manually\n",
        "    delta = df['PX_LAST'].diff()\n",
        "    gain = np.where(delta > 0, delta, 0)\n",
        "    loss = np.where(delta < 0, -delta, 0)\n",
        "\n",
        "    avg_gain = pd.Series(gain).rolling(window=14, min_periods=1).mean()\n",
        "    avg_loss = pd.Series(loss).rolling(window=14, min_periods=1).mean()\n",
        "\n",
        "    rs = avg_gain / (avg_loss + 1e-10)  # Avoid division by zero\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # Compute Momentum (5-day return)\n",
        "    df['Momentum'] = df['PX_LAST'].pct_change(periods=5)\n",
        "\n",
        "    # Initialize Signal Column\n",
        "    df['Signal'] = 0\n",
        "\n",
        "    # Generate Signals\n",
        "    for date in anomalyDates:\n",
        "        date = pd.to_datetime(date)\n",
        "        if date in df.index:\n",
        "            sma5 = df.loc[date, 'SMA_5']\n",
        "            sma20 = df.loc[date, 'SMA_20']\n",
        "            rsi = df.loc[date, 'RSI']\n",
        "            close = df.loc[date, 'PX_LAST']\n",
        "            upper_bb = df.loc[date, 'BB_Upper']\n",
        "            lower_bb = df.loc[date, 'BB_Lower']\n",
        "            momentum = df.loc[date, 'Momentum']\n",
        "\n",
        "            # Trend-Following: Buy if SMA5 > SMA20 (Uptrend), Sell if SMA5 < SMA20 (Downtrend)\n",
        "            if sma5 > sma20:\n",
        "                signal = 1  # Buy\n",
        "            elif sma5 < sma20:\n",
        "                signal = -1  # Sell\n",
        "            else:\n",
        "                signal = 0  # No clear trend\n",
        "\n",
        "            # Mean Reversion Filters\n",
        "            if close >= upper_bb or rsi > 70:\n",
        "                signal = -1  # Overbought, Sell\n",
        "            elif close <= lower_bb or rsi < 30:\n",
        "                signal = 1  # Oversold, Buy\n",
        "\n",
        "            # Momentum Confirmation (Only take trade if past returns support it)\n",
        "            if momentum > 0 and signal == 1:\n",
        "                signal = 0  # Avoid buying into strong price rise\n",
        "            elif momentum < 0 and signal == -1:\n",
        "                signal = 0  # Avoid selling into strong decline\n",
        "\n",
        "            df.loc[date, 'Signal'] = signal\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "PxsWxZs-IVER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quality of Trade"
      ],
      "metadata": {
        "id": "OoOEgD0OIhmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_returns(df, anomaly_dates, months=[3, 6]):\n",
        "    \"\"\"\n",
        "    Compute 3-month and 6-month returns from anomaly dates, selecting the next available date if missing.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame with 'PX_LAST' prices and datetime index.\n",
        "    anomaly_dates (list): List of anomaly dates (as strings or datetime).\n",
        "    months (list): List of months for return calculation (default: [3, 6]).\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Table with anomaly dates, 3M & 6M returns.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df.index = pd.to_datetime(df.index)  # Ensure index is datetime\n",
        "    sorted_dates = df.index.sort_values()  # Ensure index is sorted\n",
        "    results = []\n",
        "\n",
        "    for date in anomaly_dates:\n",
        "        anomaly_date = pd.to_datetime(date)\n",
        "        if anomaly_date not in df.index:\n",
        "            continue  # Skip if anomaly date is missing\n",
        "\n",
        "        price_at_anomaly = df.loc[anomaly_date, \"PX_LAST\"]\n",
        "        row = {\"Anomaly Date\": anomaly_date, \"Price at Anomaly\": price_at_anomaly}\n",
        "\n",
        "        for m in months:\n",
        "            future_date = anomaly_date + pd.DateOffset(months=m)\n",
        "\n",
        "            # Find the next available date if the exact future date is missing\n",
        "            future_idx = sorted_dates.searchsorted(future_date)\n",
        "            if future_idx < len(sorted_dates):  # Ensure index is within bounds\n",
        "                adjusted_future_date = sorted_dates[future_idx]\n",
        "                future_price = df.loc[adjusted_future_date, \"PX_LAST\"]\n",
        "                return_m = (future_price - price_at_anomaly) * 100 / price_at_anomaly\n",
        "                row[f\"{m}M Return\"] = return_m\n",
        "            else:\n",
        "                row[f\"{m}M Return\"] = None  # No future data available\n",
        "\n",
        "        results.append(row)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_predictions(returns_df):\n",
        "    \"\"\"\n",
        "    Compute statistics to evaluate anomaly-based predictions.\n",
        "\n",
        "    Parameters:\n",
        "    returns_df (pd.DataFrame): DataFrame containing 'Ticker', 'Anomaly Date', '3M Return', '6M Return'.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary table with counts of strong movements.\n",
        "    \"\"\"\n",
        "    stats = []\n",
        "\n",
        "    for ticker, group in returns_df.groupby(\"Ticker\"):\n",
        "        total_anomalies = len(group)\n",
        "\n",
        "        # Count occurrences where absolute return exceeds threshold\n",
        "        count_abs_3m_10 = (group[\"3M Return\"].abs() > 10).sum()\n",
        "        count_abs_6m_10 = (group[\"6M Return\"].abs() > 10).sum()\n",
        "        count_abs_3m_20 = (group[\"3M Return\"].abs() > 20).sum()\n",
        "        count_abs_6m_20 = (group[\"6M Return\"].abs() > 20).sum()\n",
        "\n",
        "        stats.append({\n",
        "            \"Ticker\": ticker,\n",
        "            \"Total Anomalies\": total_anomalies,\n",
        "            \"|3M Return| > 10%\": count_abs_3m_10,\n",
        "            \"|6M Return| > 10%\": count_abs_6m_10,\n",
        "            \"|3M Return| > 20%\": count_abs_3m_20,\n",
        "            \"|6M Return| > 20%\": count_abs_6m_20\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(stats)\n"
      ],
      "metadata": {
        "id": "xQA18rt1I25W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Volume normalization (DON)"
      ],
      "metadata": {
        "id": "Ovp6dbw_I5BA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMez0i1UIYmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Remove index component (NEED INDEX TIME SERIES)"
      ],
      "metadata": {
        "id": "BVyRmcUcI_TU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "niGRd6F4IYuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use all features (DONE)"
      ],
      "metadata": {
        "id": "6pLHcew5JNt0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lbghi9AJIY0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wXAYnS-YIY7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough"
      ],
      "metadata": {
        "id": "niJEeJ04JVMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TICKER = 'SBIN'\n",
        "PRICE_TYPE = 'PX_LAST'\n",
        "prices = getStockPriceVolumeData(TICKER, start=date(2010, 1, 1))[PRICE_TYPE]\n",
        "prices.rename('price', inplace=True)\n",
        "prices.dropna(inplace=True) # only 2 nan dates for reliance for eg."
      ],
      "metadata": {
        "id": "9bGlCVmF7vhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should we smoothen the data ?\n",
        "X, index_map = normalize_window_roll(prices,30)"
      ],
      "metadata": {
        "id": "doFTHgEEHF45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(prices.shape)\n",
        "print(len(index_map))"
      ],
      "metadata": {
        "id": "bNNufukEHL9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7*X.shape[0])\n",
        "test_size = int(0.1*X.shape[0])\n",
        "val_size = int(0.2*X.shape[0])\n",
        "\n",
        "X_train = X[:train_size]\n",
        "X_val = X[train_size+test_size:train_size+val_size+test_size]\n",
        "X_test = X[train_size:train_size+test_size]\n",
        "\n",
        "val_index_offset = train_size + test_size\n",
        "test_index_offset = train_size\n",
        "#X_val = X[train_size+test_size:train_size+test_size+val_size]"
      ],
      "metadata": {
        "id": "t1k9-2JkHHIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Training the model: The autoencoder is trained using the stock market data using the fit function.\n",
        "An EarlyStopping callback is added to stop the training if the validation loss does not improve for 2 consecutive epochs.\n",
        "'''\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                  patience=2,\n",
        "                                                  mode='min')\n",
        "autoencoder.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "history = autoencoder.fit(X_train,X_train, epochs=80,\n",
        "                    validation_data=(X_val, X_val),\n",
        "                    batch_size=16,\n",
        "                    callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "AsaIrJEHAM0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Source: https://www.tensorflow.org/tutorials/generative/autoencoder\n",
        "reconstructions = autoencoder.predict(X_train)\n",
        "train_loss = tf.keras.losses.mae(reconstructions, X_train)\n",
        "\n",
        "plt.hist(train_loss[None,:], bins=50)\n",
        "plt.xlabel(\"Train loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4xzB2xBHAVxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = np.mean(train_loss) + np.std(train_loss)\n",
        "print(\"Threshold: \", threshold)"
      ],
      "metadata": {
        "id": "dOC-U-49AYbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(train_loss)"
      ],
      "metadata": {
        "id": "36BvYktT2yBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructions = autoencoder.predict(X_test)\n",
        "test_loss = tf.keras.losses.mae(reconstructions, X_test)\n",
        "\n",
        "plt.hist(test_loss[None, :], bins=50)\n",
        "plt.xlabel(\"Test loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LzY5MAKMAanu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = predict(autoencoder, X_test, threshold)"
      ],
      "metadata": {
        "id": "HWFDAvtcAdqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "begoAU-flEMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalous_test_data = X_test[np.where(preds==False)]"
      ],
      "metadata": {
        "id": "uvgONzNgAhNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data = autoencoder.encoder(anomalous_test_data).numpy()\n",
        "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
        "\n",
        "for p in [1,10,20]:\n",
        "  plt.plot(anomalous_test_data[p], 'b')\n",
        "  plt.plot(np.arange(0,X.shape[1]),decoded_data[p], 'r')\n",
        "  plt.fill_between(np.arange(X.shape[1]), decoded_data[p], anomalous_test_data[p], color='lightcoral')\n",
        "  plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Cl_QkdnLAj8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize on original price plot"
      ],
      "metadata": {
        "id": "mOePXDW7ZN6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anomalousIndexIds  = np.where(preds == False)[0]\n",
        "anomalousIndexIds = [ int(test_index_offset + id) for id in anomalousIndexIds ]"
      ],
      "metadata": {
        "id": "QEPL1s93ZSln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dotDates = [date for i in anomalousIndexIds for date in index_map[i]]\n",
        "dotDates = [index_map[i][0] for i in anomalousIndexIds]"
      ],
      "metadata": {
        "id": "W3b6_2rLbRmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dotDates"
      ],
      "metadata": {
        "id": "iYpmIOMV-ThF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VuIxb_OaJ8oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plotPrice(TICKER, 'PX_LAST', date(2013, 1, 1), None, dotDates, lineDates=[index_map[val_index_offset][0], index_map[test_index_offset][0]])"
      ],
      "metadata": {
        "id": "uqoyE8sMnD5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker"
      ],
      "metadata": {
        "id": "UySQYx3WnM_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices.index[test_index_offset]"
      ],
      "metadata": {
        "id": "fW0mzjZVncb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_index_offset"
      ],
      "metadata": {
        "id": "SAwrJbugFoqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_index_offset"
      ],
      "metadata": {
        "id": "GOoJjnItFrOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kg1LpBmFtTL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}